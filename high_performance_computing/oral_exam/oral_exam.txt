Notes for oral exam in HPC 2020

Exam topic: Impact of synchronization between compute units in OpenCL. 

Specific question: Optimal utilization of GPU work-groups by tuning local size

Good resources: 
	Discussion on local size - https://stackoverflow.com/a/3959831
	Optimizing local memory - https://stackoverflow.com/a/28537318
	Nvidia Opencl Best practices - https://www.nvidia.com/content/cudazone/CUDABrowser/downloads/papers/NVIDIA_OpenCL_BestPracticesGuide.pdf
	OpenCL book: http://asu-cs.donntu.org/sites/default/files/images/doc/opencl.programming.guide.pdf

Naive approach: No local size, i.e. the local work group size will be chosen by the compiler.

Our approach: We just set the local size to {10, 10}

Optimization: Try to optimize the local size. Note that the local size 
	      should be a multiple of the global size, and preferably 
	      a multiple of 32/64 as well. 
	      Local size is the amount of work per workgroup, i.e. per SM (streaming multiprocessor)
	- Another optimization: Minimize data transfers to host. Perform average
	  calculations on GPU as well.

Nvidia threads are queued up in groups of warps, i.e. 32 threads. Thus a local size 
of a multiple of 32 would utilize the group size more efficiently. This is the case for
our Quadro P4000 (Warp size 32). 

From NVidia best practices:

"If the GPU processor must wait on one warp of threads, it simply 
  begins executing work on another."

"High Priority: Use the effective bandwidth of your computation 
 as a metric when measuring performance and optimization benefits."


*** Note! Modify all programs to have profiling command queue - may give some extra overhead

*********** Benchmarks **********
All benchmark performed using hyperfine, with 2 warmup iterations.

Overhead calculations corresponds to setting up all buffers, writing buffers etc.,
as well as calculating averages. It also includes the profiling command queue.
However, it does not include the calculation loop.

Baseline: (s)  		 100x100	 100000x100	  10000x10000		128x128		100000x128	10016x10016		
	Overhead:


	Naive:  	1.584+-0.034	 4.241+-0.019	  112.462+-0.126


	Handin: 	1.462+-0.014	 1.556+-0.023	  32.154+-0.029


Local size 
Experiments: (s)  100x100	 100000x100	  10000x10000		128x128		100000x128	10016x10016		
	1x1		
	2x2	
	4x4										
	5x5
	8x8
	10x10
	15x15
	16x16
	20x20
	25x25:	 		 1.695+-0.014	  30.621+-0.048		
	32x32:	  DOES NOT RUN	 DOES NOT RUN     DOES NOT RUN




Averages computed on GPU - based on handin implementation
Experiments: (s)  100x100	 100000x100	  10000x10000


Load local size tile into shared memory - based on handin implementatiom
Experiments: (s)  100x100	 100000x100	  10000x10000

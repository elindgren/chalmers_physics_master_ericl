{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: Eric Lindgren\n",
    "#### CID: ericlin\n",
    "#### PSN: 970222-1954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (n_data, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "                       with labels (n_data) = (1797,)\n",
      "\n",
      "flattened input, X: (n_inputs, n_features)  = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import \n",
    "from sklearn import datasets\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(f\"inputs: (n_data, pixel_width, pixel_height) = {inputs.shape}\")\n",
    "print(f\"                       with labels (n_data) = {labels.shape}\")\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(f\"\\nflattened input, X: (n_inputs, n_features)  = {inputs.shape}\")\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f\"Label: {digits.target[random_indices[i]]:1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1257\n",
      "Number of test images:      540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "print(f\"Number of training images: {len(X_train):4}\")\n",
    "print(f\"Number of test images:     {len(X_test):4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical turns our integer vector into a onehot representation\n",
    "# we implement it in in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Full object oriented implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            X_data,\n",
    "            Y_data,\n",
    "            n_hidden_neurons=50,\n",
    "            n_categories=10,\n",
    "            epochs=10,\n",
    "            batch_size=100,\n",
    "            eta=0.1,\n",
    "            lmbd=0.0):\n",
    "\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1./(1. + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def softmax(z):\n",
    "        '''Softmax activation function for the output layer.'''\n",
    "        ax=0\n",
    "        if isinstance(z[0], np.ndarray):\n",
    "            'Check if a list, if so use matrix version of argmax.'\n",
    "            ax=1\n",
    "        return np.exp(z)/((np.exp(z)).sum(axis=ax)).reshape(-1,1)\n",
    "\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.hidden_weights = np.random.randn(self.n_features, self.n_hidden_neurons)\n",
    "        self.hidden_bias = np.zeros(self.n_hidden_neurons) + 0.01\n",
    "\n",
    "        self.output_weights = np.random.randn(self.n_hidden_neurons, self.n_categories)\n",
    "        self.output_bias = np.zeros(self.n_categories) + 0.01\n",
    "\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        # feed-forward for training\n",
    "        # Hidden layer\n",
    "        self.z_h = self.X_data_full@self.hidden_weights + self.hidden_bias.T  # z_h\n",
    "        self.a_h = sigmoid(z=self.z_h)  # a_h\n",
    "        # Output layer\n",
    "        self.z_o = self.a_h@self.output_weights + self.output_bias.T\n",
    "        self.a_0 = softmax(self.z_o)\n",
    "\n",
    "        \n",
    "    def backpropagation(self):\n",
    "        raise NotImplementedError(\"The backpropagation method does not yet include the regularizer.\")\n",
    "        \n",
    "        error_output = self.probabilities - self.Y_data\n",
    "        error_hidden = np.matmul(error_output, self.output_weights.T) * self.a_h * (1 - self.a_h)\n",
    "\n",
    "        self.output_weights_gradient = np.matmul(self.a_h.T, error_output)\n",
    "        self.output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "        self.hidden_weights_gradient = np.matmul(self.X_data.T, error_hidden)\n",
    "        self.hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "        # Add the weight gradients from the regularizer term.\n",
    "        # Add code here\n",
    "        self.hidden_weights_gradient += lmbd*self.hidden_weights_gradient\n",
    "        self.output_weights_gradient += lmbd*self.output_weights_gradient\n",
    "\n",
    "        self.output_weights -= self.eta * self.output_weights_gradient\n",
    "        self.output_bias -= self.eta * self.output_bias_gradient\n",
    "        self.hidden_weights -= self.eta * self.hidden_weights_gradient\n",
    "        self.hidden_bias -= self.eta * self.hidden_bias_gradient\n",
    "\n",
    "\n",
    "    def fixed_feed_forward(self, X):\n",
    "        '''\n",
    "        Performs a feed formward pass for input data X. For use exclusively by the predict and \n",
    "        predict_probabilities functions.\n",
    "        '''\n",
    "        z_h = X@self.hidden_weights + self.hidden_bias.T  # z_h\n",
    "        a_h = sigmoid(z=self.z_h)  # a_h\n",
    "        # Output layer\n",
    "        z_o = self.a_h@self.output_weights + self.output_bias.T\n",
    "        a_o = softmax(self.z_o)\n",
    "\n",
    "        return a_o\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Return a vector of predictions for input X'''\n",
    "        probabilities = fixed_feed_forward(X)\n",
    "        if isinstance(probabilities[0], np.ndarray):\n",
    "            'Check if a list, if so use matrix version of argmax.'\n",
    "            class_label = probabilities.argmax(axis=1)  # Returns a number between 0-9 - the index\n",
    "        else: \n",
    "            class_label = probabilities.argmax()\n",
    "        return class_label\n",
    "\n",
    "        \n",
    "    def predict_probabilities(self, X):\n",
    "        '''Return all of the probabilities for input X'''\n",
    "        probabilities = fixed_feed_forward(X)\n",
    "        return probabilities\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                # pick datapoints with replacement\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                # minibatch training data\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta=0.1\n",
    "lmbd=0.\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "dnn = NeuralNetwork(X_train, Y_train_onehot, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                    n_hidden_neurons=n_hidden_neurons, n_categories=n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fixed_feed_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0e162e91ebed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy on training data before training: {accuracy_score(dnn.predict(X_train), Y_train):.3f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-31ebf19c64c7>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;34m'''Return a vector of predictions for input X'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfixed_feed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;34m'Check if a list, if so use matrix version of argmax.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fixed_feed_forward' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on training data before training: {accuracy_score(dnn.predict(X_train), Y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: Eric Lindgren\n",
    "#### CID: ericlin\n",
    "#### PSN: 970222-1954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (n_data, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "                       with labels (n_data) = (1797,)\n",
      "\n",
      "flattened input, X: (n_inputs, n_features)  = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import \n",
    "from sklearn import datasets\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(f\"inputs: (n_data, pixel_width, pixel_height) = {inputs.shape}\")\n",
    "print(f\"                       with labels (n_data) = {labels.shape}\")\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(f\"\\nflattened input, X: (n_inputs, n_features)  = {inputs.shape}\")\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f\"Label: {digits.target[random_indices[i]]:1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1257\n",
      "Number of test images:      540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "print(f\"Number of training images: {len(X_train):4}\")\n",
    "print(f\"Number of test images:     {len(X_test):4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement the feed-forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward pass as described in the problem description. The activation for layer h is:\n",
    "$$ z_j = \\sum_i^F w_{ji}x_i + b_j = W$$\n",
    "$$ \\vec{z} = W^T\\vec{x} + \\vec{b}$$\n",
    "\n",
    "Note the transpose on W, which is required the way it is designed (n_features, n_hidden_neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''Sigmoid activation function for hidden layer.'''\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    '''Softmax activation function for the output layer.'''\n",
    "    return np.exp(z)/(np.exp(z)).sum()\n",
    "\n",
    "\n",
    "def feed_forward(X):\n",
    "    \"\"\"\n",
    "    Feed-forward pass.\n",
    "    Uses hidden_weights, hidden_bias, output_weights, output_bias\n",
    "    \n",
    "    Args:\n",
    "        X (array[float]): input to the neural network\n",
    "\n",
    "    Returns:\n",
    "        a_h (array[float]): activation in the hidden layer\n",
    "        probabilities (array[float]): probabilities of each category\n",
    "    \"\"\"\n",
    "    # First hidden layer\n",
    "    z_h = np.matmul(hidden_weights.T, X) + hidden_bias\n",
    "    a_h = sigmoid(z_h)\n",
    "    # Output layer\n",
    "    z_o = np.matmul(output_weights.T, a_h) + output_bias\n",
    "    probabilities = softmax(z_o)  # Output probabilities\n",
    "    \n",
    "    return a_h, probabilities\n",
    "\n",
    "\n",
    "# prediction of class label\n",
    "def predict(X):\n",
    "    \"\"\"\n",
    "    Return a prediction by finding the class with the highest likelihood.\n",
    "\n",
    "    Args:\n",
    "        X (array[float]): input to the neural network\n",
    "\n",
    "    Returns:\n",
    "        label (integer): index of the category with the highest probability\n",
    "    \"\"\"\n",
    "\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    \n",
    "    # Modify the code below\n",
    "    #\n",
    "    class_label = probabilities.argmax()  # Returns a number between 0-9 - the index\n",
    "    return class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input of a 1 the network outputed a 7. Was this correct? False.\n",
      "For input of a 2 the network outputed a 7. Was this correct? False.\n",
      "For input of a 3 the network outputed a 6. Was this correct? False.\n",
      "For input of a 4 the network outputed a 7. Was this correct? False.\n",
      "For input of a 5 the network outputed a 7. Was this correct? False.\n",
      "For input of a 6 the network outputed a 6. Was this correct? True.\n",
      "For input of a 7 the network outputed a 1. Was this correct? False.\n",
      "For input of a 8 the network outputed a 9. Was this correct? False.\n",
      "For input of a 9 the network outputed a 7. Was this correct? False.\n"
     ]
    }
   ],
   "source": [
    "# Test by feeding the input data\n",
    "# Take a few indices and see what we get out\n",
    "indices = [3, 23, 1, 64, 4, 2, 11, 8, 7]  # Indices for one of each\n",
    "for indx in indices:\n",
    "    X_image = X_train[indx]\n",
    "    Y_label = Y_train[indx]\n",
    "    prediction = predict(X_image)\n",
    "    print(f'For input of a {Y_label} the network outputed a {prediction}. Was this correct? {Y_label==prediction}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical turns our integer vector into a onehot representation\n",
    "# we implement it in in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF345 Advanced Simulation and Machine Learning\n",
    "## Lab2\n",
    "Andreas Ekström, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computer lab is meant to familiarize you with the concept of probabilistic programming and how to use it for doing some Bayesian inference and model selection. We will use the Python module 'pymc3'. It uses Theano for computing gradients via automatic differentiation. Although it compiles probabilistic programs to C for increased speed, it allows model specification directly in Python code. \n",
    "\n",
    "From Wikipedia, we read (https://en.wikipedia.org/wiki/Probabilistic_programming)\n",
    ">Probabilistic programming (PP) is a programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty. Programming languages used for probabilistic programming are referred to as \"probabilistic programming languages\" (PPLs).\n",
    "\n",
    "One should always take claims of \"...performed automatically...\" and \"...widely applicable...\" with a grain of salt. Nevertheless, the PP tools offered via e.g. pymc3, or Stan (https://mc-stan.org) which is another very popular PPL, can be very useful for doing Bayesian inference with highly non-trivial, and user-defined, probabilistic models. \n",
    "\n",
    "There are additional pymc3 examples in the second notebook 'many_models.ipynb, and the pymc3 website contains useful tutorials (https://docs.pymc.io/nb_tutorials/index.html) and further syntax documentation (https://docs.pymc.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, norm\n",
    "\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set(font='sans-serif')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import corner\n",
    "import theano.tensor as tt\n",
    "\n",
    "print(f'pymc3 version: {pm.__version__}')\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first example: linear regression\n",
    "\n",
    "You should use pymc3 to model some outcomes $Y$ as a normally distributed variable with a mean $\\mu$ and variance $\\sigma^2$. The mean is a _linear_ linear function of two parameters $\\theta_0$ and $\\theta_1$ (where we sometimes refer to $\\theta_0$ as the bias or constant) and a control parameter $x$. The control parameter might be a time, temperature, location etc, for which we have data $Y$. \n",
    "\n",
    "We write our model:\n",
    "\n",
    "\\begin{equation}\n",
    "Y \\sim \\mathcal{N}(\\mu,\\sigma^2) \\\\\n",
    "\\mu = \\theta_0 + \\theta_1 x\n",
    "\\end{equation}\n",
    "\n",
    "We will model the observational error with $\\sigma$, and employ rather weak priors\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i \\sim \\mathcal{N}(0,10^2) \\\\\n",
    "\\sigma^2 \\sim \\mathcal{IG}(1,1).\n",
    "\\end{equation}\n",
    "\n",
    "## Generate the data\n",
    "\n",
    "We can use scipy.stats to generate some fake data. We will set the true values $[\\theta_0,\\theta_1] = [1,2]$ ,consider $x \\in [-1,1]$, and add (homoskedastic) zero-mean noise distributed as $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of data points\n",
    "Nd = 50\n",
    "#true parameter values\n",
    "thetaT = np.array([1,2])\n",
    "\n",
    "x = np.linspace(-1,1,Nd)\n",
    "x_array = np.linspace(-1,1,200)\n",
    "sigma = 1.0\n",
    "\n",
    "#here we use the scipy stats norm to generate the noise\n",
    "Y = thetaT[0] + thetaT[1]*x + norm(0,sigma).rvs(Nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x,Y,label='Data');\n",
    "sns.lineplot(x_array,thetaT[0] + thetaT[1]*x_array,color='red',label='Reality',lw=2)\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the statistical model (priors and the likelihood) in pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This PPL object is a container for all _random_ variables that belong to the model\n",
    "our_regression_model = pm.Model()\n",
    "\n",
    "# everything within this Python 'context' will be added to our_regression_model\n",
    "# and can be handled by pymc3. You cannot create a random variable outside of a model\n",
    "#\n",
    "# For example: try creating a random variable by uncommenting the following line, you should get an error\n",
    "# a_random_variable = pm.Uniform('same_name_as_the_python_variable',0,1)\n",
    "#\n",
    "with our_regression_model:\n",
    "\n",
    "    # Priors for unknown model parameters:\n",
    "    # an array of two normally distributed model parameters\n",
    "    theta = pm.Normal('theta', mu=0, sd=10, shape=2)\n",
    "#     theta = pm.Uniform('theta', lower=2, upper=3, shape=2)\n",
    "    # the inverse gamma prior for the variance\n",
    "    sigma2 = pm.InverseGamma('sigma2',1,1)\n",
    "\n",
    "    # Expected value of model outcome:\n",
    "    # in our case, the linear model we use to explain the data\n",
    "    # This part of the calculation is deterministic in the sense that for\n",
    "    # given values of theta[:], the resulting mu will be computed\n",
    "    mu = theta[0] + theta[1]*x\n",
    "\n",
    "    # Likelihood of observations: this is a stochastic variable.\n",
    "    # We use a standard normal likelihood. Non-standard likelihoods\n",
    "    # must be explicitly encoded. It is important to remember all normalization constants.\n",
    "    # We did this in the many_models notebook via the DensityDist API. \n",
    "    # You can also use the Potential API.\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=tt.sqrt(sigma2), observed=Y)\n",
    "    \n",
    "    #We are done specifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a (local) MAP using the pymc3 built-in autodiff functionality and BFGS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Having specified the prior and likelihood, we can start operating on the posterior.\n",
    "#pymc3 offers built-in optimizers to find a (local) maximum a posteriori.\n",
    "#By default, find_MAP uses the Broyden–Fletcher–Goldfarb–Shanno (BFGS) optimization \n",
    "#algorithm to find the maximum of the log-posterior but also allows selection of \n",
    "#other optimization algorithms from the scipy.optimize module\n",
    "#\n",
    "#pymc3 automatically differentiates you model using automatic differentiation\n",
    "#(a rather circular statement...). Autodiff is a method for automatically generating\n",
    "#code for the derivatives of a specified function. It is not the same as \n",
    "#symbolic differentiation. Instead, via the chain-rule and operator overloading,\n",
    "#the autodiff algorithm will construct a fully differentiated function. E.g. it will\n",
    "#replace np.sin(x) with np.cos(x) throughout the backend code.\n",
    "#\n",
    "map_estimate = pm.find_MAP(model=our_regression_model)\n",
    "\n",
    "print(f'According to the Likelihood and Prior defined above, the BFGS optimizer found the following (local) MAP')\n",
    "th0 = map_estimate['theta'][0]\n",
    "th1 = map_estimate['theta'][1]\n",
    "s2  = map_estimate['sigma2']\n",
    "\n",
    "print(f'[theta_0,theta_1] = [{th0:.3f},{th1:.3f}] and [sigma2] = {s2:.3f}')\n",
    "print(f'\\nThe true parameter values are [theta_0,theta_1] = {thetaT} and [sigma2] = {sigma**2:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us turn to the true advantage of pymc3: the built-in MCMCM sampler.\n",
    "# Amongst other, you can select NUTS, Metropolis, or SMC\n",
    "# NUTS: the HMC No U-Turn Sampler. This is a gradient-based sampling method originally constructed \n",
    "# in the lattice field-theory community. It uses Hamiltonian dynamics to take long/uncorrelated Metropolis steps.\n",
    "#\n",
    "# NUTS is the default method for MCMC sampling\n",
    "with our_regression_model:\n",
    "    trace_NUTS = pm.sample(1000)\n",
    "    \n",
    "# Once you have tried NUTS, please try 1000 sequential MC steps\n",
    "# This is the method we used in the lectures to extract marginal likelihoods\n",
    "# You can comment out the NUTS sampling above. They do not have to be run simultaneously.\n",
    "with our_regression_model:\n",
    "    trace_SMC = pm.sample_smc(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the Arviz summary function to get an overview of the posterior\n",
    "# it will provide you with credibility intervals for the parameters (set by hdi_prob)\n",
    "with our_regression_model:\n",
    "    print(az.summary(trace_NUTS, kind='stats', round_to=2,hdi_prob=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use one of Arviz many plot functions to inspect the traces. You should see one lineplot per trace.\n",
    "# \n",
    "# At this stage, go back to replace the theta priors with a uniform pdf \\theta ~ U(2,3),\n",
    "# and inspect the traces. Do you understand what is going on?\n",
    "###  Answer\n",
    "### The walkers tries to reach higher probability regions, but since the uniform distribution (prior) cuts off just at the \n",
    "### beginning of the interesting region.\n",
    "#\n",
    "with our_regression_model:\n",
    "    az.plot_trace(trace_NUTS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or plot only marginalized posteriors with some mean and highest_density_intervals\n",
    "with our_regression_model:\n",
    "    az.plot_posterior(trace_NUTS,figsize=(12, 6),hdi_prob=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can stack the traces and plot them using corner\n",
    "samples = np.vstack([trace_NUTS['theta'][:,k] for k in range(0,len(trace_NUTS['theta'][0]))])\n",
    "samples = np.vstack((samples,trace_NUTS['sigma2'][:])).T\n",
    "fig_corner = corner.corner(samples,labels =['th0','th1','s2'],show_titles=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the prior and posterior predictives \n",
    "\n",
    "The prior predictive pdf represent model predictions across the prior pdf, i.e. before we have looked at the data and updated the model parameter pdfs. This can be useful for getting a feeling for how the priors play out in the physical model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can easily draw from the prior/posterior predictive distributions\n",
    "with our_regression_model:\n",
    "    posterior_draws = pm.sample_posterior_predictive(trace_NUTS, var_names=[\"theta\", \"sigma2\"], \n",
    "                                                     samples=400, random_seed=123);\n",
    "    prior_draws = pm.sample_prior_predictive(var_names=[\"theta\", \"sigma2\"], \n",
    "                                                     samples=400, random_seed=123)\n",
    "# explicitly draw prior samples, one by one, and plot the model prediction\n",
    "for idx, theta_sample in enumerate(prior_draws['theta']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(prior_draws['sigma2'][idx]))\n",
    "#     epsilon=0\n",
    "    \n",
    "    plt.plot(x_array,theta_sample[0] + theta_sample[1]*x_array + epsilon,color='black',alpha=0.05)\n",
    "    \n",
    "plt.title('Prior predictive');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');\n",
    "\n",
    "fig2 = plt.figure()\n",
    "# explicitly draw posterior samples, one by one, and plot the model prediction\n",
    "post_mean_pred = np.zeros(len(x_array))\n",
    "print(post_mean_pred.shape)\n",
    "for idx, theta_sample in enumerate(posterior_draws['theta']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(posterior_draws['sigma2'][idx]))\n",
    "    mod = theta_sample[0] + theta_sample[1]*x_array + epsilon\n",
    "    post_mean_pred += mod\n",
    "    plt.plot(x_array,mod,color='black',alpha=0.05)\n",
    "post_mean_pred /= len(posterior_draws['theta'])\n",
    "sns.scatterplot(x,Y,label='Data',zorder=10);\n",
    "sns.lineplot(x_array,thetaT[0] + thetaT[1]*x_array,color='red',label='Reality',lw=2,zorder=20);\n",
    "sns.lineplot(x_array,post_mean_pred,color='blue',label='Average posterior prediction',lw=2,zorder=20);\n",
    "plt.title('Posterior predictive');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. what is the role of epsilon in the above posterior predictive plot. \n",
    "##### Answer \n",
    "Epsilon, in both cases, is a constant error term that we draw from our current estimate for the variance sigma^2. \n",
    "It is our way of explaining the measurement error (which is individual in each data point) with a common error term \n",
    "constantly shifting our model output. To do it properly, epsilon should be a vector of draws for each x-value I guess.\n",
    "But doing so would maybe lead to a better representation of the data, but not fit the model.\n",
    "2. Try removing it, and observe/explain what happens\n",
    "##### Answer\n",
    "If we remove it, we are not modelling the data uncertainty in any extent. Remember, we are drawing samples \n",
    "from the posterior for theta and sigma! \n",
    "Setting epsilon to zero is the same as marginalizing out the effect of sigma (or epsilon)? From MCMC that is the same thing. \n",
    "3. Add the posterior mean prediction to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing but with the Sequential MC and likelihood tempering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you used SMC, your trace will contain reported log-marginal likelihoods for each chain. \n",
    "#We used this for model comparison during the lecture earlier this afternoon\n",
    "print(trace_SMC.report.log_marginal_likelihood)\n",
    "\n",
    "#You can easily draw from the prior/posterior predictive distributions\n",
    "with our_regression_model:\n",
    "    posterior_draws = pm.sample_posterior_predictive(trace_SMC, var_names=[\"theta\", \"sigma2\"], \n",
    "                                                     samples=400, random_seed=123);\n",
    "    prior_draws = pm.sample_prior_predictive(var_names=[\"theta\", \"sigma2\"], \n",
    "                                                     samples=400, random_seed=123)\n",
    "# explicitly draw prior samples, one by one, and plot the model prediction\n",
    "for idx, theta_sample in enumerate(prior_draws['theta']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(prior_draws['sigma2'][idx]))\n",
    "#     epsilon=0\n",
    "    \n",
    "    plt.plot(x_array,theta_sample[0] + theta_sample[1]*x_array + epsilon,color='black',alpha=0.05)\n",
    "    \n",
    "plt.title('Prior predictive');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');\n",
    "\n",
    "fig2 = plt.figure()\n",
    "# explicitly draw posterior samples, one by one, and plot the model prediction\n",
    "post_mean_pred = np.zeros(len(x_array))\n",
    "for idx, theta_sample in enumerate(posterior_draws['theta']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(posterior_draws['sigma2'][idx]))\n",
    "    mod = theta_sample[0] + theta_sample[1]*x_array + epsilon\n",
    "    post_mean_pred += mod\n",
    "    plt.plot(x_array,mod,color='black',alpha=0.05)\n",
    "post_mean_pred /= len(posterior_draws['theta'])\n",
    "sns.scatterplot(x,Y,label='Data',zorder=10);\n",
    "sns.lineplot(x_array,thetaT[0] + thetaT[1]*x_array,color='red',label='Reality',lw=2,zorder=20);\n",
    "sns.lineplot(x_array,thetaT[0] + thetaT[1]*x_array,color='blue',label='Average posterior prediction',lw=2,zorder=20);\n",
    "plt.title('Posterior predictive');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see basically no difference by using SMC instead of NUTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A second example: is there a second sine component?\n",
    "\n",
    "Using the code below you can generate 50 data points from the two-sine model, \n",
    "\n",
    "\\begin{equation}\n",
    "y(t) = A_1 \\sin \\left[\\left( \\frac{t}{P_1} + t_1\\right)2\\pi\\right] + A_2 \\sin \\left[\\left( \\frac{t}{P_2} + t_2\\right)2\\pi\\right] \\varepsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\varepsilon$ is normally distributed noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with known variance $\\sigma^2=1$. \n",
    "\n",
    "Looking at this data (a couple of cells below), without knowing that there are in fact 2 sine components, it is _a prioi_ reasonable to employ also a one sine model for interpreting the data, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "y(t) = A_1 \\sin \\left[\\left( \\frac{t}{P_1} + t_1\\right)2\\pi\\right] + \\varepsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Your **task** is to determine, based on the data, whether one of the sine models actually explains the data better than the other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sine1(t, A1, P1, t1):\n",
    "    return A1 * np.sin((t / P1 + t1) * 2 * np.pi)\n",
    "params_sine1 = ['A1','P1','t1']\n",
    "\n",
    "def model_sine2(t, A1, P1, t1, A2, P2, t2):\n",
    "    return A1 * np.sin((t / P1 + t1) * 2 * np.pi) + A2 * np.sin((t / P2 + t2) * 2 * np.pi)\n",
    "params_sine2 = ['A1','P1','t1','A2','P2','t2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "n_data = 50\n",
    "\n",
    "# time of observations\n",
    "t = np.random.uniform(0, 5, size=n_data)\n",
    "\n",
    "# Use these parameter settings.\n",
    "A1_T = 4.2\n",
    "P1_T = 3.0\n",
    "t1_T = 0.0\n",
    "A2_T = 1.2\n",
    "P2_T = 1.2\n",
    "t2_T = 0.4\n",
    "\n",
    "yerr = 1.0\n",
    "y = np.random.normal(model_sine2(t, A1_T, P1_T, t1_T, A2_T, P2_T, t2_T), yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.errorbar(x=t, y=y, yerr=yerr, marker='o', ls=' ', color='black', ecolor='gray', ms=2)\n",
    "plt.title('Data')\n",
    "t_range = np.linspace(0, 5, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define priors and likelihoods in pymc3\n",
    "\n",
    "you can start with uniform priors $A_i \\sim \\mathcal{U}(0.5,10)$, $P_i \\sim \\mathcal{U}(0.5,5)$, and $t_i \\sim \\mathcal{U}(0,0.5)$\n",
    "\n",
    "### One-sine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_model = pm.Model()\n",
    "\n",
    "with M1_model:\n",
    "    # Params ['A1','P1','t1']\n",
    "    A1 = pm.Uniform('A1', lower=0.5, upper=10, shape=1)\n",
    "    P1 = pm.Uniform('P1', lower=0.5, upper=5, shape=1)\n",
    "    t1 = pm.Uniform('t1', lower=0, upper=0.5, shape=1)\n",
    "    \n",
    "\n",
    "    # Expected value of model outcome:\n",
    "#     mu = model_sine1(t, A, P, t1)\n",
    "    mu = A1 * np.sin((t / P1 + t1) * 2 * np.pi)\n",
    "\n",
    "    # Likelihood of observations\n",
    "    y_obs_m1 = pm.Normal('y_obs_m1', mu=mu, sd=yerr, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-sine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "M2_model = pm.Model()\n",
    "with M2_model:\n",
    "    # Params ['A1','P1','t1','A2','P2','t2']\n",
    "    A1 = pm.Uniform('A1', lower=0.5, upper=10, shape=1)\n",
    "    P1 = pm.Uniform('P1', lower=0.5, upper=5, shape=1)\n",
    "    t1 = pm.Uniform('t1', lower=0, upper=0.5, shape=1)\n",
    "    A2 = pm.Uniform('A2', lower=0.5, upper=10, shape=1)\n",
    "    P2 = pm.Uniform('P2', lower=0.5, upper=5, shape=1)\n",
    "    t2 = pm.Uniform('t2', lower=0, upper=0.5, shape=1)\n",
    "\n",
    "    # Expected value of model outcome:\n",
    "#     mu = model_sine2(t, A[0], P[0], ts[0], A[1], P[1], ts[1])\n",
    "    mu  = A1 * np.sin((t / P1 + t1) * 2 * np.pi) + A2 * np.sin((t / P2 + t2) * 2 * np.pi)\n",
    "\n",
    "    # Likelihood of observations\n",
    "    y_obs_m2 = pm.Normal('y_obs_m2', mu=mu, sd=yerr, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior predictive to inspect the prior choices\n",
    "\n",
    "### One-sine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with M1_model:\n",
    "    prior_draws = pm.sample_prior_predictive(var_names=params_sine1, \n",
    "                                             samples=400, random_seed=123)\n",
    "# explicitly draw prior samples, one by one, and plot the model prediction\n",
    "\n",
    "for idx, A_sample in enumerate(prior_draws['A1']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(yerr))\n",
    "    plt.plot(t_range,model_sine1(t_range, A_sample, prior_draws['P1'][idx], prior_draws['t1'][idx]) + epsilon,color='black',alpha=0.05) \n",
    "    \n",
    "sns.scatterplot(t,y,label='Data',zorder=10);\n",
    "sns.lineplot(t_range,model_sine1(t_range, A1_T, P1_T, t1_T) ,color='red',label='Reality',lw=2,zorder=20);\n",
    "    \n",
    "plt.title('Prior predictive, model 1');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-sine model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with M2_model:\n",
    "    prior_draws = pm.sample_prior_predictive(var_names=params_sine2, \n",
    "                                             samples=400, random_seed=123)\n",
    "# explicitly draw prior samples, one by one, and plot the model prediction\n",
    "\n",
    "for idx, A_sample in enumerate(prior_draws['A1']):\n",
    "#     epsilon = np.random.normal(scale=np.sqrt(prior_draws['sigma2'][idx]))\n",
    "    epsilon = np.random.normal(scale=np.sqrt(yerr))\n",
    "    plt.plot(t_range,model_sine2(t_range, A_sample, prior_draws['P1'][idx], prior_draws['t1'][idx], prior_draws['A2'][idx], prior_draws['P2'][idx], prior_draws['t2'][idx] + epsilon),color='black',alpha=0.05)\n",
    "    \n",
    "plt.title('Prior predictive, model 2');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample posteriors and compute $\\log(Z_{M_i})$ with SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc_sample(pymc3_model, N=4000):\n",
    "#As with other sampling methods, running a sampler more than one time is useful \n",
    "#to compute diagnostics, SMC is no exception. PyMC3 will try to run at least two \n",
    "#SMC chains. Not to confuse with the N Markov chains inside each SMC chain).\n",
    "    with pymc3_model:\n",
    "        trace = pm.sample_smc(draws=N,chains=2)\n",
    "    return trace, trace.report.log_marginal_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample sine model 1\n",
    "M1_trace, M1_report = smc_sample(M1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample sine model 2\n",
    "M2_trace, M2_report = smc_sample(M2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trace(trace,params,CI_alpha=0.1):\n",
    "    print(az.summary(trace, round_to=2, kind='stats',hdi_prob=(1-CI_alpha)))\n",
    "    axs = az.plot_trace(trace,legend=True);   \n",
    "    samples = np.vstack(np.array([trace[k] for k in params]).T)\n",
    "    fig_corner = corner.corner(samples,labels = params, show_titles=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check traces\n",
    "check_trace(M1_trace, params_sine1)\n",
    "check_trace(M2_trace, params_sine2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect marginal likelihoods\n",
    "with M1_model:\n",
    "    az.plot_posterior(M1_trace,figsize=(12, 6),hdi_prob=0.9);\n",
    "with M2_model:\n",
    "    az.plot_posterior(M2_trace,figsize=(12, 6),hdi_prob=0.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot posterior predictives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_T = 4.2\n",
    "P1_T = 3.0\n",
    "t1_T = 0.0\n",
    "A2_T = 1.2\n",
    "P2_T = 1.2\n",
    "t2_T = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "with M1_model:\n",
    "    posterior_draws = pm.sample_posterior_predictive(M1_trace, var_names=params_sine1, \n",
    "                                                     samples=400, random_seed=123);\n",
    "# explicitly draw posterior samples, one by one, and plot the model prediction\n",
    "post_mean_pred = np.zeros(len(t_range))\n",
    "for idx, A1 in enumerate(posterior_draws['A1']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(yerr))\n",
    "    mod = model_sine1(t_range, A1, posterior_draws['P1'][idx], posterior_draws['t1'][idx]) + epsilon\n",
    "    post_mean_pred += mod\n",
    "    plt.plot(t_range,mod,color='black',alpha=0.05)\n",
    "post_mean_pred /= len(posterior_draws['A1'])\n",
    "\n",
    "sns.scatterplot(t,y,label='Data',zorder=10);\n",
    "sns.lineplot(t_range,model_sine2(t_range, A1_T, P1_T, t1_T, A2_T, P2_T, t2_T) ,color='red',label='Reality',lw=2,zorder=20);\n",
    "sns.lineplot(t_range,post_mean_pred,color='blue',label='Average posterior prediction',lw=2,zorder=20);\n",
    "plt.title('Posterior predictive - M1');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "with M2_model:\n",
    "    posterior_draws = pm.sample_posterior_predictive(M2_trace, var_names=params_sine2, \n",
    "                                                     samples=400, random_seed=123);\n",
    "# explicitly draw posterior samples, one by one, and plot the model prediction\n",
    "post_mean_pred = np.zeros(len(t_range))\n",
    "for idx, A1 in enumerate(posterior_draws['A1']):\n",
    "    epsilon = np.random.normal(scale=np.sqrt(yerr))\n",
    "    mod = model_sine2(t_range, A1, posterior_draws['P1'][idx], posterior_draws['t1'][idx], posterior_draws['A2'][idx], posterior_draws['P2'][idx], posterior_draws['t2'][idx]) + epsilon\n",
    "    post_mean_pred += mod\n",
    "    plt.plot(t_range,mod,color='black',alpha=0.05)\n",
    "post_mean_pred /= len(posterior_draws['A1'])\n",
    "\n",
    "sns.scatterplot(t,y,label='Data',zorder=10);\n",
    "sns.lineplot(t_range,model_sine2(t_range, A1_T, P1_T, t1_T, A2_T, P2_T, t2_T) ,color='red',label='Reality',lw=2,zorder=20);\n",
    "sns.lineplot(t_range,post_mean_pred,color='blue',label='Average posterior prediction',lw=2,zorder=20);\n",
    "plt.title('Posterior predictive - M2');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We see M1 explaining the data fairly well, once the error bands are included. However, M2 is of course a lot better, almost perfectly matching the reality and following the data really nicely. \n",
    "\n",
    "The marginal distributions hone in on the true values nicely (which we ofc can't know). Interestingly, in the M2 case we see that the parameters A, P and t are multimodal, which encompass the fact that our model is made up of two sines, and thus the role of the sines are interchangable given the parameter values. However,\n",
    "this means that A, P and t are very correlated in this case which is seen in the corner plots; i.e. interchaning A1 with A2 means that we've got to switch both P and t as well. If we didn't need to do this, then our corner plots would have modes at e.g (A1, P2) as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A third example: N-dimensional double-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test the capability of SMC to sample $\\log(Z)$ of a bi-modal posterior in some parameter vector $\\boldsymbol{\\theta}$. We set up this problem using a double-Gaussian likelihood, with identical but known covariance $\\boldsymbol{\\Sigma}$, and a single-Gaussian parameter prior. This implies that we characterize our data with some vector of two means $[\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2]$, each one describing the mean of a Gaussian. Bayes' theorem relates all our quantites\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{\\theta}|[\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2]) = \\frac{p([\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2]|\\boldsymbol{\\theta},\\boldsymbol{\\Sigma})p(\\boldsymbol{\\theta}|\\boldsymbol{\\mu}_0,\\boldsymbol{\\Sigma}_0)}{p([\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2])},\n",
    "\\end{equation}\n",
    "\n",
    "where the likelihood is given by\n",
    "\n",
    "\\begin{equation}\n",
    "p([\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2]|\\boldsymbol{\\theta},\\boldsymbol{\\Sigma},\\delta) = \\delta\\mathcal{N}(\\boldsymbol{\\mu}|\\boldsymbol{\\theta},\\boldsymbol{\\Sigma}) + (1-\\delta)\\mathcal{N}(\\boldsymbol{\\mu}|\\boldsymbol{\\theta},\\boldsymbol{\\Sigma}) = \\\\\\\\\n",
    "\\frac{\\delta}{(2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left\\{ -\\frac{1}{2}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\theta})^T \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\theta})\\right\\} + \\frac{1-\\delta}{(2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left\\{ -\\frac{1}{2}(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\theta})^T \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\theta})\\right\\} \n",
    "\\end{equation}\n",
    "\n",
    "and the prior is given by\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{\\theta}|\\boldsymbol{\\mu}_0,\\boldsymbol{\\Sigma}_0) = \\mathcal{N}(\\boldsymbol{\\theta}|\\boldsymbol{\\mu}_0,\\boldsymbol{\\Sigma}_0) = \\frac{1}{(2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}_0|^{1/2}}\\exp\\left\\{ -\\frac{1}{2}(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\theta})^T \\boldsymbol{\\Sigma}_0^{-1}(\\boldsymbol{\\mu}_0 - \\boldsymbol{\\theta})\\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "We choose this setup since a normally distributed prior is conjugate to a normally distributed likelihood, and the posterior is thus also normally distributed. \n",
    "\n",
    "One can show that the posterior is normally distributed with a covariance is given by \n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\boldsymbol{\\Sigma}} = (\\boldsymbol{\\Sigma}^{-1} + \\boldsymbol{\\Sigma}_0^{-1})^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "To obtain the marginal likelihood $p([\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2])$, we simply look at all known normalization factors. For Bayes' theorem in general, we have, in order, a prior\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{\\theta}) = \\frac{1}{Z_{0}} q(\\boldsymbol{\\theta}),\n",
    "\\end{equation}\n",
    "\n",
    "a likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{\\mathcal{D}}|\\boldsymbol{\\theta}) = \\frac{1}{Z_{\\ell}} q(\\boldsymbol{\\mathcal{D}}|\\boldsymbol{\\theta}),\n",
    "\\end{equation}\n",
    "\n",
    "and a posterior\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol{\\theta}|\\boldsymbol{\\mathcal{D}}) = \\frac{1}{Z_{\\boldsymbol{\\theta}}} q(\\boldsymbol{\\mu}|\\boldsymbol{\\mathcal{D}}), .\n",
    "\\end{equation}\n",
    "\n",
    "Where the un-normalized pdf:s are denoted $q(\\cdot)$. Therefore, we can identify\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{Z_{\\boldsymbol{\\theta}}} q(\\boldsymbol{\\mu}|\\boldsymbol{\\mathcal{D}}) = \\frac{q(\\boldsymbol{\\mathcal{D}}|\\boldsymbol{\\theta}) q(\\boldsymbol{\\theta})}{Z_{\\ell}Z_{0}p(\\boldsymbol{\\mathcal{D}})} \\Rightarrow p(\\boldsymbol{\\mathcal{D}}) = \\frac{Z_{\\boldsymbol{\\theta}}}{Z_{\\ell}Z_{0}}\n",
    "\\end{equation}\n",
    "\n",
    "In our case, with normal distributions throughout, we have\n",
    "\n",
    "\\begin{equation}\n",
    "Z_0 = (2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}_0|^{1/2},\\,\\,\\,\\, Z_{\\ell} = (2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}|^{1/2},\\,\\,\\,\\, Z_{\\boldsymbol{\\theta}} = (2\\pi)^{N_p/2}|\\tilde{\\boldsymbol{\\Sigma}}|^{1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "The analytical result for the posterior becomes\n",
    "\n",
    "\\begin{equation}\n",
    "Z \\equiv p(\\boldsymbol{\\mathcal{D}}) = \\frac{|\\tilde{\\boldsymbol{\\Sigma}}|^{1/2}}{(2\\pi)^{N_p/2}|\\boldsymbol{\\Sigma}|^{1/2}|\\boldsymbol{\\Sigma}_0|^{1/2}} \\Rightarrow \\log[Z] = \\frac{1}{2}\\log[|\\tilde{\\boldsymbol{\\Sigma}}|] - \\frac{N_p}{2}\\log[2\\pi] - \\frac{1}{2}\\log[|\\boldsymbol{\\Sigma}|] - \\frac{1}{2}\\log[|\\boldsymbol{\\Sigma}_0|].\n",
    "\\end{equation}\n",
    "\n",
    "This result remains the same for a likelihood constructed as a linear combination of two normal likelihoods with identical covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Use the write-up above, and the code below to answer the following questions:\n",
    "0. Do you follow the overarching logic behind deriving the analytical expression for the marginal likelihood?\n",
    "1. Compare the SMC computation of $\\log[Z]$ in $N_p=1$ and $N_p=3$ dimensions. \n",
    "2. Inspect the traces, do the results agree with your expectations? (two gaussians of equal size...)\n",
    "3. Try with $N_p=20$. What happens? How many gaussians do you find? How do they compare?\n",
    "4. The relative distributions are wrong, but the total marginal likelihood is rather good. What happened? Is this a problem?!\n",
    "4. Go back to $N_p=3$, and launch the HMC-NUTS samples (the default pymc3 method). How many gaussians do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we put two gaussian in Np dimensions\n",
    "Np = 3\n",
    "\n",
    "#they are symmetrically located around zero\n",
    "mu1 = np.ones(Np) * (1)\n",
    "mu2 = -mu1\n",
    "\n",
    "sig2_prior = 10**2\n",
    "Sigma_prior = sig2_prior*np.eye(Np)\n",
    "Sigma_prior_inv = np.linalg.inv(Sigma_prior)\n",
    "Sigma_prior_det = np.linalg.det(Sigma_prior)\n",
    "logZ_prior = (Np/2)*np.log(2*np.pi)+(1/2)*np.log(Sigma_prior_det)\n",
    "\n",
    "sig2 = 0.05**2\n",
    "Sigma_likelihood = sig2*np.eye(Np)\n",
    "#below you can try a random covariance\n",
    "#S = np.random.rand(Np,Np)\n",
    "#Sigma_likelihood = sig2*(S.T@S)\n",
    "\n",
    "Sigma_likelihood_inv = np.linalg.inv(Sigma_likelihood)\n",
    "Sigma_likelihood_det = np.linalg.det(Sigma_likelihood)\n",
    "logZ_likelihood = (Np/2)*np.log(2*np.pi)+(1/2)*np.log(Sigma_likelihood_det)\n",
    "\n",
    "Sigma_posterior = np.linalg.inv(Sigma_likelihood_inv + Sigma_prior_inv)\n",
    "Sigma_posterior_inv = np.linalg.inv(Sigma_posterior)\n",
    "Sigma_posterior_det = np.linalg.det(Sigma_posterior)\n",
    "\n",
    "logZ_posterior = (Np/2)*np.log(2*np.pi)+(1/2)*np.log(Sigma_posterior_det)\n",
    "\n",
    "print(f'log[Z_prior] = {logZ_prior}')\n",
    "print(f'log[Z_likelihood] = {logZ_likelihood}')\n",
    "print(f'log[Z_posterior] = {logZ_posterior}')\n",
    "\n",
    "log_marginal_likelihood = logZ_posterior - logZ_prior - logZ_likelihood\n",
    "print(f'log[marginal-likelihood] = {log_marginal_likelihood}')\n",
    "\n",
    "delta = 0.5  #  with delta of the probability mass in mode [1], mode [2] will get (1-delta) of the probability mass\n",
    "\n",
    "def double_gaussians(theta):\n",
    "    log_likelihood_G1 = (-0.5*(Np*tt.log(2*np.pi)+tt.log(Sigma_likelihood_det)+\n",
    "                               (theta-mu1).T.dot(Sigma_likelihood_inv).dot(theta-mu1)))\n",
    "    log_likelihood_G2 = (-0.5*(Np*tt.log(2*np.pi)+tt.log(Sigma_likelihood_det)+\n",
    "                               (theta-mu2).T.dot(Sigma_likelihood_inv).dot(theta-mu2)))\n",
    "    return pm.math.logsumexp([tt.log(delta) + log_likelihood_G1, tt.log(1-delta) + log_likelihood_G2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_gaussian_model = pm.Model()\n",
    "\n",
    "with double_gaussian_model:\n",
    "    theta = pm.Normal(\"theta\", mu=np.zeros_like(mu1), sd = np.sqrt(sig2_prior), shape=Np)\n",
    "    log_likelihood = pm.Potential(\"log_likelihood\", double_gaussians(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with double_gaussian_model:\n",
    "#     trace_double_gaussian = pm.sample_smc(2000)\n",
    "    \n",
    "    #try sampling also with a more conventional MCMC samples (HMC-NUTS)\n",
    "    trace_double_gaussian = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with double_gaussian_model:\n",
    "    print(az.summary(trace_double_gaussian, kind='diagnostics',round_to=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with double_gaussian_model:\n",
    "    az.plot_trace(trace_double_gaussian);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trace_double_gaussian.report.log_marginal_likelihood)\n",
    "print(f'SMC   log[marginal-likelihood] = {np.mean(trace_double_gaussian.report.log_marginal_likelihood):.2f}')\n",
    "print(f'EXACT log[marginal-likelihood] = {log_marginal_likelihood:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can stack the traces and plot them in corner\n",
    "samples = np.vstack([trace_double_gaussian['theta'][:,k] for k in range(0,len(trace_double_gaussian['theta'][0]))]).T\n",
    "fig_corner = corner.corner(samples,show_titles=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra tasks and more: Bayesian object detection\n",
    "\n",
    "With pymc3 you can construct a purely Bayesian approach to detecting discrete signals in a noisy 2d data frame, as long as they are well separated signals. This example is hypothetical, but one could imagine a scenario where researchers have obtained data from some space telescope, and the locations of potential stars are unknown.\n",
    "\n",
    "This example is taken from https://arxiv.org/pdf/0704.3704.pdf, where also multimodal Nested Sampling (MultiNest) is introduced. You can already now test this approach, for clear and well-separated signals, using the pymc3 SMC sampler, however it does not perform as well as MultiNest when the parameter landscape gets more complicated.\n",
    "\n",
    "If you would like to move forward and play with this idea, I can recommend to download and install a python version of Multinest from Johannes Buchner's github: https://github.com/JohannesBuchner/PyMultiNest\n",
    "\n",
    "Johannes also offers a code, called UltraNest, which has a better Jupyter Notebook interface: https://johannesbuchner.github.io/UltraNest/index.html\n",
    "\n",
    "An example of a next-generation algorithm is PolyChord, with a lite-version available via github:\n",
    "https://github.com/PolyChord/PolyChordLite\n",
    "(I have not used this myself yet...)\n",
    "\n",
    "The SMC implementation might run a bit slow on the JupyterHub, and would therefore recommend to run on a dedicated machine. In particular if you would like to explore Nested Sampling with many live points and a high-resolution data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code returns an XY-grid with gaussian 2d signal with amplitude A, Radius R, and mean located at X0,Y0\n",
    "#the data is a 1d vector to simplify future model comparison\n",
    "def gaussian_shape(A,X0,Y0,R,XY):\n",
    "    mu=(XY-[X0,Y0])**2/(-2*R**2)\n",
    "    return np.array(A*np.exp(np.sum(mu,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0 ; xmax = 10\n",
    "ymin = 0 ; ymax = 10\n",
    "\n",
    "Nx = 60\n",
    "Ny = 60\n",
    "\n",
    "xscale = Nx/(xmax-xmin)\n",
    "yscale = Ny/(ymax-ymin)\n",
    "\n",
    "Xline = np.linspace(xmin, xmax, Nx)\n",
    "Yline = np.linspace(ymin, ymax, Ny)\n",
    "X, Y = np.meshgrid(Xline, Yline)\n",
    "XY = np.dstack((X, Y))\n",
    "XY_arr = XY.reshape(-1,2)\n",
    "\n",
    "Rmin = 0.3 ; Rmax = 0.4\n",
    "Amin = 0.5 ; Amax = 2.0\n",
    "\n",
    "# how many objects do you want the fake data to have?\n",
    "Nobjects = 3\n",
    "objects = []\n",
    "for object in range(0,Nobjects):\n",
    "    R = Rmin + np.random.rand()*(Rmax-Rmin)\n",
    "    A = Amin + np.random.rand()*(Amax-Amin)\n",
    "    X0 = xmin + np.random.rand()*(xmax-xmin)\n",
    "    Y0 = ymin + np.random.rand()*(ymax-ymin)\n",
    "    objects.append([X0,Y0,A,R])\n",
    "\n",
    "#objects = [[2,3,1,1],[4,7,1,1],[8,2,1,1]]\n",
    "    \n",
    "signal = np.zeros((Nx*Ny))\n",
    "print(f'{Nobjects} Object located at [X,Y,A,R]:')\n",
    "for object in objects:\n",
    "    print(object)\n",
    "    X0 = object[0]\n",
    "    Y0 = object[1]\n",
    "    A  = object[2]\n",
    "    R  = object[3]\n",
    "    signal += gaussian_shape(A,X0,Y0,R,XY_arr)\n",
    "    \n",
    "noise_rms = 0.5\n",
    "noise = np.random.normal(0,noise_rms,(Nx*Ny))\n",
    "\n",
    "# you can remove the white noise by commenting the last term out\n",
    "data = signal + noise\n",
    "\n",
    "# plot the data\n",
    "ax = sns.heatmap(data.reshape(Nx,Ny),cmap='magma');\n",
    "ax.set_xticks(np.linspace(0, Nx, 5))\n",
    "ax.set_xticklabels(np.linspace(xmin, xmax, 5))\n",
    "ax.set_yticks(np.linspace(0, Ny, 5))\n",
    "ax.set_yticklabels(np.linspace(ymin, ymax, 5))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('X');\n",
    "ax.set_ylabel('Y');\n",
    "ax.set_title('Data');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

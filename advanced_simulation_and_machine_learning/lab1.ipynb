{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF345 Advanced Simulation and Machine Learning\n",
    "## Lab1\n",
    "Andreas Ekstr√∂m, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this computer lab is to:\n",
    "1. numerically verify some marginalized posteriors in Bayesian regression \n",
    "2. construct your own Gaussian process (GP) from scratch\n",
    "3. test/apply your own GP to a simple 1d linear regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import invgamma, t, norm\n",
    "# for plotting \n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "sns.set(font='sans-serif')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginalizing posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a few necessary distributions\n",
    "#perhaps helpful \n",
    "def normal_distribution(mu,sigma2):\n",
    "    return norm(loc=mu,scale=np.sqrt(sigma2))\n",
    "\n",
    "def ig_distribution(alpha,beta):\n",
    "    return invgamma(a=alpha,scale=beta)\n",
    "\n",
    "def t_distribution(nu,mu,sigma_hat2):\n",
    "    return t(df=nu,loc=mu,scale=np.sqrt(sigma_hat2))\n",
    "\n",
    "def nig_pdf(th,s2,mu0,Sigma0,alpha,beta):\n",
    "\n",
    "    if alpha<=0:\n",
    "        print('error alpha<=0')\n",
    "        return 0\n",
    "    if beta<=0:\n",
    "        print('error beta<=0')\n",
    "        return 0\n",
    "    \n",
    "    NIG = norm.pdf(th,loc=mu0,scale=np.sqrt(s2)*np.sqrt(Sigma0))*invgamma.pdf(s2,a=alpha,scale=beta) \n",
    "    return NIG  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## <font color='red'>Exercise: </font>\n",
    "plot the pdf:s for a zero-mean normal- and T-distributions with $\\sigma^2=2$. What happens when you increase the number of degrees of freedom for the T-distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "# Norm\n",
    "x = np.linspace(norm.ppf(0.01),\n",
    "                norm.ppf(0.99), 100)\n",
    "axs[0].plot(x, normal_distribution(0, 2).pdf(x))\n",
    "# T\n",
    "df = 2\n",
    "x = np.linspace(t.ppf(0.01, df),\n",
    "                t.ppf(0.99, df), 100)\n",
    "axs[1].plot(x, t_distribution(df, 0, 2).pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normal-inverse gamma (NIG) prior\n",
    "\n",
    "A NIG prior is conjugate to a likelihood with unknown $\\boldsymbol \\theta$ and $\\sigma^2$, and is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\boldsymbol\\theta,\\sigma^2) = p(\\boldsymbol\\theta|\\sigma^2)p(\\sigma^2) = \\mathcal{N}(\\boldsymbol \\theta| \\mathbf{\\mu}_0,\\sigma^2 \\boldsymbol \\Sigma)\\cdot\\mathcal{IG}(\\sigma^2|\\alpha,\\beta) \\equiv \\mathcal{NIG}(\\boldsymbol \\theta, \\sigma^2 |\\boldsymbol \\mu,\\boldsymbol \\Sigma,\\alpha,\\beta).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "### One can show that marginal distributions are given by\n",
    "\\begin{align}\n",
    "  \\begin{split}\n",
    "    p(\\boldsymbol{\\theta}) {}& =\\int \\mathcal{NIG}(\\boldsymbol{\\theta},\\sigma^2|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma},\\alpha,\\beta)\\,d\\sigma^2 = \\mathcal{T}_{2\\alpha}(\\boldsymbol{\\theta}|\\boldsymbol{\\mu},(\\beta/\\alpha)\\boldsymbol{\\Sigma})\\\\\n",
    "    p(\\sigma^2) {}& = \\int \\mathcal{NIG}(\\boldsymbol{\\theta},\\sigma^2|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma},\\alpha,\\beta)\\,d\\boldsymbol{\\theta} = \\mathcal{IG}(\\sigma^2|\\alpha,\\beta).\n",
    "  \\end{split}\n",
    "\\end{align}\n",
    "\n",
    ">## <font color='red'>Exercise: </font>\n",
    "Limit yourself to the 1-dimensional case (i.e. all vector and matrix input reduces to scalar input), and verify the above marginals by plotting the ($\\theta$-) $\\sigma^2$-integrated NIG pdf on top of the scipy-generated pdf for the (normal-) t-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "theta = np.linspace(-10, 10, 100)\n",
    "s2 = np.linspace(0.01, 20, 100)\n",
    "mu0 = 1\n",
    "Sigma0 = 1\n",
    "alpha = 1\n",
    "beta = 1\n",
    "NIG = np.zeros((len(theta), len(s2)))\n",
    "for i, th in enumerate(theta):\n",
    "    for j, s in enumerate(s2):\n",
    "        NIG[i, j] = nig_pdf(th, s, mu0, Sigma0, alpha, beta) \n",
    "p_s2 = np.trapz(y=NIG, x=theta, axis=0)\n",
    "p_theta = np.trapz(y=NIG, x=s2, axis=1)\n",
    "print(np.trapz(y=p_s2, x=s2))\n",
    "print(np.trapz(y=p_theta, x=theta))\n",
    "\n",
    "# Plot marginal distributions\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,9))\n",
    "axs[0].plot(theta, p_theta, label=r'$p(\\theta)$')\n",
    "axs[0].plot(theta, t(2*alpha, mu0, beta/alpha*Sigma0).pdf(theta), label=r'$\\tau_{2\\alpha}(\\theta)$')\n",
    "axs[1].plot(s2, p_s2, label=r'$p(\\sigma^2)$')\n",
    "axs[1].plot(s2, ig_distribution(alpha, beta).pdf(s2), label=r'$\\mathcal{IG}(\\sigma^2)$')\n",
    "axs[0].legend(loc='best')\n",
    "axs[1].legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## <font color='red'>Exercise: </font>\n",
    "What is the reason for the fatter tails of the t-distribution compared to the normal distribution? How can you intuitively explain why it reduces to the normal distribution for $\\nu \\rightarrow \\infty$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-distribution is something we use when we don't know the standard deviation $\\sigma^2$. The $\\nu$ is how \"certain\" we are on the standard deviation $\\sigma^2$. \n",
    "We can motivate that we should obtain a normal distribution (i.e. an explicit $\\sigma^2$) from the equation for $p(\\theta)$ which is a t-distribution: if we see $IG$ as a \n",
    "prior on $\\sigma^2$, in the limit $\\alpha \\rightarrow \\infty$ this will be infinitely peaked (a delta function). Thus we will pick out the normal distribution at a specific $\\sigma^2$, which \n",
    "thus says that is the limit for the t-distribution. I.e. we are sure about the standard deviation, and we should get a normal distribution.\n",
    "\n",
    "Following this logic, we should get fatter tailes for the t-distribution than the normal distribution, since we have an uncertainty in the standard deviation (\"width\"), and thus we would expect the plot \n",
    "to cover more values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will implement your own code for a 'vanilla' Gaussian process that you also condition on some data, i.e. specific points $(x_i,y_i)$. There exists highly optimized packages for using GP, but it is instructive to have implemented a GP from scratch at least once your life :)\n",
    "\n",
    "You should use a squared exponential kernel with correlation length $\\ell_{GP}=1$, variance $\\sigma_{GP}^2=1$. Ignore the stage of optimizing hyperparameters. (feel free to do it if you'd like...).The review by Rasmussen and Williams is an excellent online resource for GP theory (http://www.gaussianprocess.org/gpml/chapters/RW.pdf) \n",
    "\n",
    "In a second step you should add incorporate some data noise (with variance $\\sigma^2=0.1$) to the GP kernel. This way you can reproduce the GP 'waists' around the data points in the figure below. Indeed, if you look closely at the GP, you will notice that the GP variance does not shrink to zero in the vicinity of the (red) noise-free data points.\n",
    "\n",
    "![alt text](GP_out.png \"Conditioned GP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some data from a cosine function (used above)\n",
    "np.random.seed(1) # use this seed to get the same data as in the image above\n",
    "Nd = 3\n",
    "\n",
    "xd = np.random.uniform(-3.,3.,(Nd,1))\n",
    "yd = np.cos(xd)\n",
    "yd_error = yd\n",
    "\n",
    "###### generate the true function ######\n",
    "xt = np.linspace(-5.,5.,100).reshape(-1,1)\n",
    "yt = np.cos(xt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty-print matrices as a helper. If you need to print and debug small matrices...\n",
    "def matprint(mat, fmt=\"g\"):\n",
    "    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
    "    for x in mat:\n",
    "        for i, y in enumerate(x):\n",
    "            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
    "        print(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X1, X2, l_gp, s_gp):\n",
    "     return s_gp * np.exp( -0.5 / l_gp**2 * np.abs(X1-X2.T)**2)\n",
    "\n",
    "# GP posterior: i.e. GP prior confronted with data {x_data,y_data}\n",
    "def gp_posterior(x_pred, x_data, y_data, l_gp=1.0, s_gp = 1.0):\n",
    "    sigma_xx = kernel(x_pred, x_pred, l_gp, s_gp)\n",
    "    sigma_xd = kernel(x_pred, x_data, l_gp, s_gp)\n",
    "    sigma_dx = kernel(x_data, x_pred, l_gp, s_gp)\n",
    "    sigma_dd = kernel(x_data, x_data, l_gp, s_gp) + 0.1\n",
    "    print(sigma_xx.shape)\n",
    "    print(sigma_xd.shape)\n",
    "    print(sigma_dx.shape)\n",
    "    print(sigma_dd.shape)\n",
    "    mu = (sigma_xd@np.linalg.inv(sigma_dd))@y_data\n",
    "    cov = sigma_xx - sigma_xd@np.linalg.inv(sigma_dd)@sigma_dx\n",
    "    return mu.reshape(-1,1), cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, draw some samples from the GP posterior\n",
    "X = np.linspace(-5, 5, 100).reshape(-1,1)\n",
    "\n",
    "\n",
    "mu, cov = gp_posterior(X, xd, yd_error, 1, 1) # Add noise\n",
    "samples = np.random.multivariate_normal(mu.ravel(), cov, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "for sample in samples:\n",
    "    ax.plot(X, sample, color='dodgerblue', alpha=0.05)\n",
    "    ax.plot(X, mu, label=r'$\\mu(x)$')\n",
    "    # Plot 1 sigma band\n",
    "#     ax.fill_between(X.ravel(), mu.ravel()-np.sqrt(np.diag(cov)).ravel(), mu.ravel()+np.sqrt(np.diag(cov)).ravel(), alpha=0.005, color='k' )\n",
    "ax.plot(xt, yt, c='r')\n",
    "ax.scatter(xd, yd, c='r')\n",
    "plt.show()\n",
    "# And plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## <font color='red'>Exercise: </font>\n",
    "Why does the GP revert to the mean after a few correlation lengths?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: compare with GPy to test your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from IPython.display import display\n",
    "kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "m = GPy.models.GPRegression(xd,yd,kernel)\n",
    "m.Gaussian_noise.variance = 0.01\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([0,1,2])\n",
    "X2 = np.array([1,2,3])\n",
    "print(np.outer(np.abs(X1-X2), np.abs(X1-X2)) )\n",
    "print(np.abs(X1-X2.T)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
